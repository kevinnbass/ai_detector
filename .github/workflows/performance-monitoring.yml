name: Performance Monitoring

on:
  schedule:
    # Run performance monitoring every 4 hours
    - cron: '0 */4 * * *'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of performance test to run'
        required: true
        default: 'full'
        type: choice
        options:
        - full
        - quick
        - load
        - stress
      duration:
        description: 'Test duration in minutes'
        required: false
        default: '10'
        type: string

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '18'

jobs:
  performance-monitoring:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt
        npm ci
    
    - name: Download previous performance data
      uses: actions/download-artifact@v3
      continue-on-error: true
      with:
        name: performance-history
        path: performance-history/
    
    - name: Start monitoring services
      run: |
        python src/api/server.py &
        API_PID=$!
        echo "API_PID=$API_PID" >> $GITHUB_ENV
        sleep 10
    
    - name: Run performance monitoring
      run: |
        TEST_TYPE="${{ github.event.inputs.test_type || 'full' }}"
        DURATION="${{ github.event.inputs.duration || '10' }}"
        
        case $TEST_TYPE in
          "quick")
            python tests/performance/run_performance_tests.py --quick
            ;;
          "load")
            python tests/performance/run_performance_tests.py \
              --config tests/performance/performance-config.json \
              --duration-minutes $DURATION
            ;;
          "stress")
            python tests/performance/run_performance_tests.py \
              --config tests/performance/performance-config.json \
              --stress-test \
              --duration-minutes $DURATION
            ;;
          *)
            python tests/performance/run_performance_tests.py \
              --config tests/performance/performance-config.json
            ;;
        esac
    
    - name: Analyze performance trends
      run: |
        python -c "
        import json
        import os
        from datetime import datetime
        from pathlib import Path
        
        # Load current results
        results_dir = Path('test-results/performance')
        current_file = next(results_dir.glob('performance_test_report_*.json'), None)
        
        if not current_file:
            print('No current performance results found')
            exit(1)
        
        with open(current_file) as f:
            current_results = json.load(f)
        
        # Load historical data
        history_dir = Path('performance-history')
        history_file = history_dir / 'performance_history.json'
        
        if history_file.exists():
            with open(history_file) as f:
                history = json.load(f)
        else:
            history = {'runs': []}
        
        # Add current results to history
        history['runs'].append({
            'timestamp': datetime.now().isoformat(),
            'commit': os.environ.get('GITHUB_SHA', 'unknown'),
            'results': current_results
        })
        
        # Keep only last 100 runs
        history['runs'] = history['runs'][-100:]
        
        # Save updated history
        history_file.parent.mkdir(exist_ok=True)
        with open(history_file, 'w') as f:
            json.dump(history, f, indent=2)
        
        # Analyze trends
        if len(history['runs']) >= 2:
            prev_run = history['runs'][-2]
            curr_run = history['runs'][-1]
            
            print('Performance Trend Analysis:')
            print('=' * 40)
            
            # Compare key metrics
            metrics_to_compare = [
                ('detection_time', 'avg_time_ms'),
                ('api_response', 'p95_time_ms'),
                ('throughput', 'operations_per_second')
            ]
            
            for metric_name, key in metrics_to_compare:
                prev_val = prev_run['results'].get(metric_name, {}).get(key, 0)
                curr_val = curr_run['results'].get(metric_name, {}).get(key, 0)
                
                if prev_val and curr_val:
                    change = ((curr_val - prev_val) / prev_val) * 100
                    trend = '📈' if change > 5 else '📉' if change < -5 else '📊'
                    print(f'{trend} {metric_name}: {change:+.2f}% change')
        
        print(f'Performance monitoring completed at {datetime.now()}')
        "
    
    - name: Check performance thresholds
      run: |
        python -c "
        import json
        from pathlib import Path
        
        # Load results
        results_dir = Path('test-results/performance')
        results_file = next(results_dir.glob('performance_test_report_*.json'), None)
        
        if results_file:
            with open(results_file) as f:
                results = json.load(f)
            
            compliance = results.get('requirement_compliance', {})
            
            if compliance.get('overall_status') == 'FAIL':
                print('❌ Performance degradation detected!')
                
                # Create issue if performance is degraded
                with open('performance_issue.md', 'w') as f:
                    f.write('# Performance Degradation Alert\n\n')
                    f.write(f'Performance monitoring detected degradation at {results[\"test_run_info\"][\"start_time\"]}\n\n')
                    f.write('## Failed Checks:\n')
                    
                    for check_name, check_data in compliance.get('checks', {}).items():
                        if check_data['status'] == 'FAIL':
                            f.write(f'- **{check_name}**: {check_data}\n')
                    
                    f.write('\n## Recommendations:\n')
                    for rec in results.get('system_monitoring', {}).get('recommendations', []):
                        f.write(f'- {rec}\n')
                
                exit(1)
            else:
                print('✅ Performance within acceptable thresholds')
        "
    
    - name: Create performance issue (if degraded)
      if: failure()
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          
          if (fs.existsSync('performance_issue.md')) {
            const body = fs.readFileSync('performance_issue.md', 'utf8');
            
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `Performance Degradation - ${new Date().toISOString().split('T')[0]}`,
              body: body,
              labels: ['performance', 'bug', 'monitoring']
            });
          }
    
    - name: Cleanup services
      if: always()
      run: |
        kill $API_PID || true
    
    - name: Upload performance history
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: performance-history
        path: performance-history/
    
    - name: Upload current performance results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: performance-monitoring-results
        path: test-results/performance/

  performance-dashboard-update:
    runs-on: ubuntu-latest
    needs: [performance-monitoring]
    if: always()
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Download performance data
      uses: actions/download-artifact@v3
      with:
        name: performance-history
        path: performance-history/
    
    - name: Generate performance dashboard
      run: |
        python -c "
        import json
        import matplotlib.pyplot as plt
        import pandas as pd
        from datetime import datetime
        from pathlib import Path
        
        # Load performance history
        history_file = Path('performance-history/performance_history.json')
        
        if not history_file.exists():
            print('No performance history available')
            exit(0)
        
        with open(history_file) as f:
            history = json.load(f)
        
        if len(history['runs']) < 2:
            print('Insufficient data for dashboard')
            exit(0)
        
        # Extract time series data
        timestamps = []
        detection_times = []
        throughputs = []
        memory_usage = []
        
        for run in history['runs'][-30:]:  # Last 30 runs
            timestamps.append(datetime.fromisoformat(run['timestamp']))
            
            # Extract metrics (with fallbacks)
            results = run['results']
            detection_times.append(
                results.get('detection_benchmarks', [{}])[0].get('avg_time_ms', 0)
            )
            throughputs.append(
                results.get('load_test', {}).get('operations_per_second', 0)
            )
            memory_usage.append(
                results.get('system_monitoring', {}).get('metrics_summary', {})
                .get('memory_usage_percent', {}).get('mean', 0)
            )
        
        # Create dashboard plots
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        
        # Detection time trend
        ax1.plot(timestamps, detection_times, 'b-o')
        ax1.set_title('Detection Time Trend')
        ax1.set_ylabel('Time (ms)')
        ax1.tick_params(axis='x', rotation=45)
        
        # Throughput trend
        ax2.plot(timestamps, throughputs, 'g-o')
        ax2.set_title('Throughput Trend')
        ax2.set_ylabel('Ops/Second')
        ax2.tick_params(axis='x', rotation=45)
        
        # Memory usage trend
        ax3.plot(timestamps, memory_usage, 'r-o')
        ax3.set_title('Memory Usage Trend')
        ax3.set_ylabel('Usage %')
        ax3.tick_params(axis='x', rotation=45)
        
        # Performance score (synthetic)
        performance_scores = []
        for i, _ in enumerate(timestamps):
            score = 100
            if detection_times[i] > 100: score -= 20
            if throughputs[i] < 16.7: score -= 20
            if memory_usage[i] > 80: score -= 20
            performance_scores.append(max(0, score))
        
        ax4.plot(timestamps, performance_scores, 'm-o')
        ax4.set_title('Overall Performance Score')
        ax4.set_ylabel('Score (0-100)')
        ax4.tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.savefig('performance_dashboard.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # Generate summary report
        with open('performance_summary.md', 'w') as f:
            f.write('# Performance Monitoring Dashboard\n\n')
            f.write(f'Last updated: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n\n')
            f.write('## Recent Performance Trends\n\n')
            
            latest = history['runs'][-1]['results']
            f.write(f'- **Detection Time**: {detection_times[-1]:.2f} ms\n')
            f.write(f'- **Throughput**: {throughputs[-1]:.2f} ops/sec\n')
            f.write(f'- **Memory Usage**: {memory_usage[-1]:.2f}%\n')
            f.write(f'- **Performance Score**: {performance_scores[-1]:.0f}/100\n\n')
            
            # Status indicators
            compliance = latest.get('requirement_compliance', {})
            status = compliance.get('overall_status', 'UNKNOWN')
            status_emoji = '✅' if status == 'PASS' else '❌' if status == 'FAIL' else '❓'
            f.write(f'**Current Status**: {status_emoji} {status}\n\n')
            
            if status == 'FAIL':
                f.write('### Issues Detected:\n')
                for check_name, check_data in compliance.get('checks', {}).items():
                    if check_data['status'] == 'FAIL':
                        f.write(f'- {check_name}: {check_data}\n')
        
        print('Performance dashboard generated successfully')
        "
      continue-on-error: true
    
    - name: Upload dashboard
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: performance-dashboard
        path: |
          performance_dashboard.png
          performance_summary.md