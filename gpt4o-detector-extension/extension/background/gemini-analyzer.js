/**
 * GEMINI LLM ANALYZER FOR CHROME EXTENSION
 * =======================================
 * 
 * This module integrates Google's Gemini API with the Chrome extension
 * to provide real-time AI detection on X/Twitter posts with structured analysis.
 */

class GeminiAnalyzer {
    constructor() {
        this.apiKey = null;
        this.modelName = 'gemini-1.5-flash'; // Fast and cost-effective
        this.baseUrl = 'https://generativelanguage.googleapis.com/v1beta/models';
        this.rateLimiter = new RateLimiter(1, 1000); // 1 request per second
        this.cache = new Map(); // Cache results to avoid re-analysis
        this.maxCacheSize = 1000;
        
        this.initializeApiKey();
    }

    /**
     * Initialize API key from storage
     */
    async initializeApiKey() {
        try {
            const result = await chrome.storage.local.get(['geminiApiKey']);
            if (result.geminiApiKey) {
                this.apiKey = result.geminiApiKey;
                console.log('‚úÖ Gemini API key loaded');
            } else {
                console.log('‚ö†Ô∏è No Gemini API key found - LLM analysis disabled');
            }
        } catch (error) {
            console.error('‚ùå Error loading API key:', error);
        }
    }

    /**
     * Set API key and save to storage
     */
    async setApiKey(apiKey) {
        this.apiKey = apiKey;
        await chrome.storage.local.set({ geminiApiKey: apiKey });
        console.log('‚úÖ Gemini API key saved');
    }

    /**
     * Check if API key is available
     */
    isConfigured() {
        return !!this.apiKey;
    }

    /**
     * Analyze tweet text using Gemini LLM
     */
    async analyzeTweet(tweetText, options = {}) {
        if (!this.isConfigured()) {
            throw new Error('Gemini API key not configured');
        }

        if (!tweetText || tweetText.length < 10) {
            throw new Error('Tweet text too short for analysis');
        }

        // Check cache first
        const cacheKey = this.getCacheKey(tweetText);
        if (this.cache.has(cacheKey)) {
            console.log('üìã Using cached analysis');
            return this.cache.get(cacheKey);
        }

        // Rate limiting
        await this.rateLimiter.wait();

        try {
            const startTime = Date.now();
            
            // Choose analysis type based on options
            const analysisType = options.quick ? 'quick' : 'comprehensive';
            const result = await this.performAnalysis(tweetText, analysisType);
            
            const processingTime = Date.now() - startTime;
            result.processing_time = processingTime / 1000; // Convert to seconds
            result.analysis_timestamp = new Date().toISOString();
            result.model_used = this.modelName;

            // Cache the result
            this.cacheResult(cacheKey, result);

            console.log(`üß† Tweet analyzed in ${processingTime}ms`);
            return result;

        } catch (error) {
            console.error('‚ùå Gemini analysis failed:', error);
            throw error;
        }
    }

    /**
     * Perform the actual analysis based on type
     */
    async performAnalysis(tweetText, analysisType) {
        if (analysisType === 'quick') {
            return await this.quickAnalysis(tweetText);
        } else {
            return await this.comprehensiveAnalysis(tweetText);
        }
    }

    /**
     * Quick analysis - focuses on key indicators for fast detection
     * Updated with optimized prompt based on data analysis
     */
    async quickAnalysis(tweetText) {
        const prompt = `Analyze this tweet to determine if it was generated by GPT-4o or written by a human. Focus on these proven discriminating patterns:

**PRIMARY AI INDICATORS (High Reliability):**
1. **Hedging Language** (87% reliable): Look for excessive qualifying phrases like "it's important to note", "it's worth considering", "merit thorough examination"
2. **Balanced Presentation** (85% reliable): Systematic "On one hand... On the other hand" structures presenting both sides
3. **Formal Transitions** (82% reliable): Academic phrases like "Furthermore", "Moreover", "However" in casual contexts
4. **Meta-Commentary** (79% reliable): Explicit analysis discussion like "when examining", "in considering", "we must acknowledge"

**PRIMARY HUMAN INDICATORS (High Reliability):**
1. **Casual Authenticity** (88% reliable): Natural use of "tbh", "lol", "rn", "lmao", informal abbreviations
2. **Natural Errors** (86% reliable): Inconsistent capitalization, casual grammar, authentic typos
3. **Emotional Spontaneity** (84% reliable): Genuine reactions like "I literally cannot even", "honestly it's insane"
4. **Personal Voice** (77% reliable): Direct opinions, personal anecdotes, unique perspectives

**Twitter Context Analysis:**
- Formal academic language is highly suspicious on Twitter
- Balanced arguments are less natural in social media context
- Personal anecdotes and emotional reactions are strong human signals
- Consistent formal structure indicates AI generation

TWEET: "${tweetText}"

**Scoring Guidelines:**
- 3+ AI indicators + formal tone = 0.8+ AI probability
- 2+ human indicators + casual tone = 0.8+ human probability  
- Mixed signals = 0.4-0.6 range with lower confidence
- Twitter context strongly influences final score

Return ONLY valid JSON:
{
    "ai_probability": 0.0-1.0,
    "prediction": "ai" or "human",
    "confidence": {"value": 0.0-1.0, "level": "low/medium/high"},
    "key_indicators": ["list of specific patterns found"],
    "reasoning": "brief explanation focusing on key patterns",
    "twitter_context_score": 0.0-1.0,
    "evidence": ["specific quoted phrases that match patterns"],
    "pattern_matches": {
        "ai_patterns_found": ["hedging_language", "balanced_presentation", "formal_transitions", "meta_commentary"],
        "human_patterns_found": ["casual_authenticity", "natural_errors", "emotional_spontaneity", "personal_voice"]
    }
}`;

        return await this.callGeminiAPI(prompt);
    }

    /**
     * Comprehensive analysis - full structured analysis (slower but more accurate)
     */
    async comprehensiveAnalysis(tweetText) {
        const prompt = `Perform comprehensive AI detection analysis on this tweet. Return ONLY valid JSON.

TWEET: "${tweetText}"

Analyze across all dimensions with scores 0.0 (human-like) to 1.0 (AI-like):

{
    "ai_probability": 0.0-1.0,
    "prediction": "human" or "ai",
    "overall_confidence": {
        "value": 0.0-1.0,
        "level": "low/medium/high",
        "reliability": 0.0-1.0
    },
    "dimension_scores": {
        "linguistic": {
            "score": 0.0-1.0,
            "hedging_frequency": 0.0-1.0,
            "contrast_rhetoric": 0.0-1.0,
            "formal_register": 0.0-1.0,
            "meta_commentary": 0.0-1.0
        },
        "cognitive": {
            "score": 0.0-1.0,
            "processing_consistency": 0.0-1.0,
            "complexity_distribution": 0.0-1.0,
            "attention_patterns": 0.0-1.0
        },
        "emotional": {
            "score": 0.0-1.0,
            "authenticity": 0.0-1.0,
            "granularity": 0.0-1.0,
            "natural_expression": 0.0-1.0
        },
        "creativity": {
            "score": 0.0-1.0,
            "originality": 0.0-1.0,
            "risk_taking": 0.0-1.0,
            "unique_voice": 0.0-1.0
        },
        "personality": {
            "score": 0.0-1.0,
            "voice_consistency": 0.0-1.0,
            "individual_markers": 0.0-1.0,
            "authenticity": 0.0-1.0
        }
    },
    "key_indicators": ["primary", "detection", "markers"],
    "human_markers": ["indicators", "of", "human", "authorship"],
    "ai_markers": ["indicators", "of", "ai", "generation"],
    "evidence": ["specific", "textual", "evidence"],
    "contradictions": ["any", "conflicting", "signals"],
    "twitter_specific": {
        "platform_appropriateness": 0.0-1.0,
        "casual_authenticity": 0.0-1.0,
        "social_context": 0.0-1.0,
        "emoji_usage": "natural/systematic/absent",
        "abbreviation_patterns": "authentic/formal/mixed"
    },
    "reasoning": "detailed explanation of the analysis",
    "recommendation": "high_confidence_human/likely_human/uncertain/likely_ai/high_confidence_ai",
    "suggested_action": "accept/flag_for_review/mark_as_ai"
}

Consider Twitter's casual, social context. Look for natural human expression vs systematic AI patterns.`;

        return await this.callGeminiAPI(prompt);
    }

    /**
     * Call Gemini API with the given prompt
     */
    async callGeminiAPI(prompt) {
        const url = `${this.baseUrl}/${this.modelName}:generateContent?key=${this.apiKey}`;
        
        const payload = {
            contents: [{
                parts: [{
                    text: prompt
                }]
            }],
            generationConfig: {
                temperature: 0.1,
                topK: 20,
                topP: 0.8,
                maxOutputTokens: 2000,
                responseMimeType: "application/json"
            },
            safetySettings: [
                {
                    category: "HARM_CATEGORY_HATE_SPEECH",
                    threshold: "BLOCK_NONE"
                },
                {
                    category: "HARM_CATEGORY_DANGEROUS_CONTENT", 
                    threshold: "BLOCK_NONE"
                },
                {
                    category: "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                    threshold: "BLOCK_NONE"
                },
                {
                    category: "HARM_CATEGORY_HARASSMENT",
                    threshold: "BLOCK_NONE"
                }
            ]
        };

        const response = await fetch(url, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json'
            },
            body: JSON.stringify(payload)
        });

        if (!response.ok) {
            const errorText = await response.text();
            throw new Error(`Gemini API error: ${response.status} - ${errorText}`);
        }

        const data = await response.json();
        
        if (!data.candidates || data.candidates.length === 0) {
            throw new Error('No response from Gemini API');
        }

        const responseText = data.candidates[0].content.parts[0].text;
        
        try {
            return JSON.parse(responseText);
        } catch (parseError) {
            console.error('‚ùå JSON parsing failed:', parseError);
            // Fallback: try to extract basic info
            return this.parseFallbackResponse(responseText);
        }
    }

    /**
     * Fallback parsing when JSON fails
     */
    parseFallbackResponse(responseText) {
        // Extract basic probability if possible
        const probabilityMatch = responseText.match(/(\d+(?:\.\d+)?)(?:%|\s*out of\s*(?:100|1\.0))/i);
        let aiProbability = 0.5;
        
        if (probabilityMatch) {
            aiProbability = parseFloat(probabilityMatch[1]);
            if (aiProbability > 1) {
                aiProbability = aiProbability / 100; // Convert percentage
            }
        }

        return {
            ai_probability: aiProbability,
            prediction: aiProbability > 0.5 ? "ai" : "human",
            confidence: {
                value: 0.3,
                level: "low"
            },
            key_indicators: ["parsing_error"],
            evidence: [],
            reasoning: "JSON parsing failed, using fallback analysis",
            raw_response: responseText.substring(0, 500),
            error: "Response parsing failed"
        };
    }

    /**
     * Generate cache key for a tweet
     */
    getCacheKey(tweetText) {
        // Simple hash function
        let hash = 0;
        for (let i = 0; i < tweetText.length; i++) {
            const char = tweetText.charCodeAt(i);
            hash = ((hash << 5) - hash) + char;
            hash = hash & hash; // Convert to 32-bit integer
        }
        return hash.toString();
    }

    /**
     * Cache analysis result
     */
    cacheResult(key, result) {
        // Implement LRU cache
        if (this.cache.size >= this.maxCacheSize) {
            const firstKey = this.cache.keys().next().value;
            this.cache.delete(firstKey);
        }
        
        this.cache.set(key, {
            ...result,
            cached_at: Date.now()
        });
    }

    /**
     * Clear cache
     */
    clearCache() {
        this.cache.clear();
        console.log('üßπ Analysis cache cleared');
    }

    /**
     * Get cache statistics
     */
    getCacheStats() {
        return {
            size: this.cache.size,
            maxSize: this.maxCacheSize,
            hitRate: this.cacheHits / (this.cacheHits + this.cacheMisses) || 0
        };
    }
}

/**
 * Simple rate limiter class
 */
class RateLimiter {
    constructor(maxRequests, windowMs) {
        this.maxRequests = maxRequests;
        this.windowMs = windowMs;
        this.requests = [];
    }

    async wait() {
        const now = Date.now();
        
        // Remove old requests outside the window
        this.requests = this.requests.filter(time => now - time < this.windowMs);
        
        // If we're at the limit, wait
        if (this.requests.length >= this.maxRequests) {
            const oldestRequest = Math.min(...this.requests);
            const waitTime = this.windowMs - (now - oldestRequest);
            
            if (waitTime > 0) {
                console.log(`‚è±Ô∏è Rate limiting: waiting ${waitTime}ms`);
                await new Promise(resolve => setTimeout(resolve, waitTime));
            }
        }
        
        // Add current request
        this.requests.push(Date.now());
    }
}

// Create global instance
const geminiAnalyzer = new GeminiAnalyzer();

// Export for use in background script
if (typeof module !== 'undefined' && module.exports) {
    module.exports = { GeminiAnalyzer, geminiAnalyzer };
}